{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression as lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-3-a3e47daafe8c>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-a3e47daafe8c>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    def predict(self):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#Did not get around to finishing this one yet.\n",
    "class logregression(object):\n",
    "    \n",
    "    def __init__(self,step_size = 0.5):\n",
    "        self.step_size = step_size\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        rows = np.size(X,0)\n",
    "        columns = np.size(X,1)\n",
    "        w = np.ones(rows)\n",
    "        b = 1\n",
    "        \n",
    "        for i in range(rows):\n",
    "            \n",
    "    \n",
    "    def predict(self):\n",
    "        self.predict = lr.predict(self.X)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobMatrix(X,y,w,b):\n",
    "    \n",
    "    dNLLw = np.zeros(np.size(X,1))\n",
    "    \n",
    "    #I've used a for loop in this instance because I could not get my head around the matrix form layout.\n",
    "    for j in range(np.size(X,1)):\n",
    "\n",
    "        f = np.matmul(w,np.transpose(X[j,:])) + b\n",
    "\n",
    "        dNLLw[j] = np.sum(np.multiply(-y[j]*np.transpose(X[j,:]),np.exp(-f))/(1+np.exp(-f))+np.multiply((1-y[j])*np.transpose(X[j,:]),np.exp(f))/(1+np.exp(f)))\n",
    "        #dNLLb = np.matmul(np.transpose(-y),np.exp(-f))/(1+np.exp(-f))+np.matmul(np.transpose((1-y)),np.exp(f))/(1+np.exp(f))\n",
    "        #disabled for the moment because we're not using it yet anyway.\n",
    "        \n",
    "    return dNLLw #dNLLb \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobMatrix(X,y,w,b):\n",
    "    \n",
    "    dNLLw = np.zeros(np.size(X,1))\n",
    "    dNLLb = np.zeros(np.size(X,1))\n",
    "    \n",
    "    #I've used a for loop in this instance because I could not get my head around the matrix form layout.\n",
    "    for j in range(np.size(X,1)):\n",
    "\n",
    "        f = np.matmul(w,np.transpose(X[j,:])) + b\n",
    "\n",
    "        dNLLw[j] = np.sum(np.multiply(-y[j]*np.transpose(X[j,:]),np.exp(-f))/(1+np.exp(-f))+np.multiply((1-y[j])*np.transpose(X[j,:]),np.exp(f))/(1+np.exp(f)))\n",
    "\n",
    "        \n",
    "    f = np.matmul(w,np.transpose(X)) + b\n",
    "    dNLLb = np.sum(np.multiply(-y,np.exp(-f))/(1+np.exp(-f))+np.multiply((1-y),np.exp(f))/(1+np.exp(f)))\n",
    "\n",
    "        \n",
    "    return dNLLw, dNLLb \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(4,2)\n",
    "y = np.random.randint(2,size=4)\n",
    "w = np.ones(2)\n",
    "b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL(X,y,w,b):\n",
    "    \n",
    "    f = np.matmul(w,np.transpose(X)) + b\n",
    "    \n",
    "    NLL = np.sum(y*np.log(1+np.exp(-f))+(1-y)*np.log(1+np.exp(f)))\n",
    "    \n",
    "    return NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_w(X,y,w,b,iterations,step_size = 1):\n",
    "    \n",
    "    w_old = w\n",
    "    NLL_old = NLL(X,y,w_old,b)\n",
    "    NLL_new = 0 #We start with an arbitrary difference to satisfy the first while loop. \n",
    "    k = 0\n",
    "    \n",
    "    while k < iterations and abs(NLL_new - NLL_old) > 1e-06:\n",
    "        alpha = step_size\n",
    "        w_pend = w_old - alpha*jacobMatrix(X,y,w_old,b)[0] \n",
    "        j = 0\n",
    "    \n",
    "        while NLL(X,y,w_pend,b) > NLL(X,y,w_old,b) and j < 20:\n",
    "            alpha = alpha/2\n",
    "            w_pend = w_old - alpha*jacobMatrix(X,y,w_old,b)[0]\n",
    "            j = j + 1\n",
    "        \n",
    "        w_new = w_pend\n",
    "        NLL_new = NLL(X,y,w_new,b)\n",
    "        NLL_old = NLL(X,y,w_old,b)\n",
    "        w_old = w_new\n",
    "        k = k + 1\n",
    "    \n",
    "    return w_new,k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_b(X,y,w,b,iterations,step_size = 1):\n",
    "    \n",
    "    b_old = b\n",
    "    NLL_old = NLL(X,y,w,b_old)\n",
    "    NLL_new = 0 #We start with an arbitrary difference to satisfy the first while loop. \n",
    "    k = 0\n",
    "    \n",
    "    while k < iterations and abs(NLL_new - NLL_old) > 1e-06:\n",
    "        alpha = step_size\n",
    "        b_pend = b_old - alpha*jacobMatrix(X,y,w,b_old)[1] \n",
    "        j = 0\n",
    "    \n",
    "        while NLL(X,y,w,b_pend) > NLL(X,y,w,b_old) and j < 20:\n",
    "            alpha = alpha/2\n",
    "            b_pend = b_old - alpha*jacobMatrix(X,y,w,b_old)[1]\n",
    "            j = j + 1\n",
    "        \n",
    "        b_new = b_pend\n",
    "        NLL_new = NLL(X,y,w,b_new)\n",
    "        NLL_old = NLL(X,y,w,b_old)\n",
    "        b_old = b_new\n",
    "        k = k + 1\n",
    "    \n",
    "    return b_new,k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_Descent(X,y,w,b,iterations,step_size = 1):\n",
    "    [w_new,i] = optimise_w(X,y,w,b,np.int(iterations*0.9),step_size)\n",
    "    [b_new,j] = optimise_b(X,y,w_new,b,iterations-np.int(iterations*0.9),step_size)\n",
    "    \n",
    "    NLL_new = NLL(X,y,w_new,b_new)\n",
    "    \n",
    "    k = i+j\n",
    "    \n",
    "    return NLL_new,w_new,b_new,k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6072128730805646,\n",
       " array([-6.05388592, 11.27692659]),\n",
       " -0.3093319336174671,\n",
       " 26)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_Descent(X,y,w,b,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearlySeparableData(n):\n",
    "    OFM = np.random.rand(n,2)\n",
    "    Labels = np.zeros(shape = (n,1))\n",
    "\n",
    "    for i in range(n):\n",
    "        if OFM[i,1] > 0.5+np.random.uniform(-0.1,0.1):\n",
    "            Labels[i] = 1\n",
    "        \n",
    "    return OFM, Labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70.55529693591923, array([-0.60428559,  1.64205726]), -0.9209728462626915, 13)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Q, r] = nearlySeparableData(10)\n",
    "w = np.ones(2)\n",
    "b = 0\n",
    "gradient_Descent(Q,r,w,b,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separableData(n):\n",
    "    OFM = np.random.rand(n,2)\n",
    "    Labels = np.zeros(shape = (n,1))\n",
    "\n",
    "    for i in range(n):\n",
    "        if (OFM[i,0]**2 + OFM[i,1]**2) > 0.5:\n",
    "            Labels[i] = 1\n",
    "        \n",
    "    return OFM, Labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63.21459495573542, array([1.00000027, 0.9999999 ]), -0.08644607705607113, 7)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Q, r] = separableData(10)\n",
    "w = np.ones(2)\n",
    "b = 0\n",
    "gradient_Descent(Q,r,w,b,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inseparableData(n):\n",
    "    OFM = np.random.rand(n,2)\n",
    "    Labels = np.random.randint(low = 2, size = (n,1))\n",
    "        \n",
    "    return OFM, Labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72.22912838592023,\n",
       " array([ 2.0426429 , -0.77720832]),\n",
       " -0.34248793234213964,\n",
       " 16)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Q, r] = inseparableData(10)\n",
    "w = np.ones(2)\n",
    "b = 0\n",
    "gradient_Descent(Q,r,w,b,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
